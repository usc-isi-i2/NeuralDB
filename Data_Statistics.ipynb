{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4fe6f2e",
   "metadata": {},
   "source": [
    "# Python Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a86217d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: virtualenv in /nas/home/ssingh41/anaconda3/lib/python3.9/site-packages (20.13.3)\n",
      "Requirement already satisfied: distlib<1,>=0.3.1 in /nas/home/ssingh41/anaconda3/lib/python3.9/site-packages (from virtualenv) (0.3.4)\n",
      "Requirement already satisfied: platformdirs<3,>=2 in /nas/home/ssingh41/anaconda3/lib/python3.9/site-packages (from virtualenv) (2.5.1)\n",
      "Requirement already satisfied: filelock<4,>=3.2 in /nas/home/ssingh41/anaconda3/lib/python3.9/site-packages (from virtualenv) (3.3.1)\n",
      "Requirement already satisfied: six<2,>=1.9.0 in /nas/home/ssingh41/anaconda3/lib/python3.9/site-packages (from virtualenv) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install virtualenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dffe8496",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv myenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "625cfe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source myenv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4285ee7e",
   "metadata": {},
   "source": [
    "# Support Set Generator Utils Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e17902",
   "metadata": {},
   "source": [
    "### Run this code block as it is required to run the other modules of prediction and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddf778fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "class SSG:\n",
    "\n",
    "    def read_NDB(self,data_file):\n",
    "        with open(data_file) as file:\n",
    "            dataset = []\n",
    "\n",
    "            for line in file:\n",
    "                db = json.loads(line)\n",
    "\n",
    "                facts = db[\"facts\"]\n",
    "                queries = db[\"queries\"]\n",
    "                dataset.append([facts, queries])\n",
    "            return dataset\n",
    "\n",
    "\n",
    "    def create_dataset(self,db):\n",
    "        dataset = []\n",
    "        eos = \"<eos>\"\n",
    "        for d in db:\n",
    "\n",
    "            questions = d[1]\n",
    "            ctx = d[0]\n",
    "\n",
    "            for q in questions:\n",
    "\n",
    "                t = q[\"height\"]\n",
    "                gold_facts = q[\"facts\"]\n",
    "                context = ctx[: t + 1]\n",
    "                flat_facts = [item for sublist in gold_facts for item in sublist]\n",
    "\n",
    "                # all facts in flat facts can be positive\n",
    "                state = [q[\"query\"]]\n",
    "                pos_act = [context[g] for g in flat_facts]\n",
    "                # everything else is negative\n",
    "                neg_act = [x for i, x in enumerate(context) if i not in flat_facts]\n",
    "\n",
    "                dataset.append([state, eos, 0])\n",
    "                dataset.extend([[state, n, 0] for n in neg_act])\n",
    "                pos_set = [[state, p, 1] for p in pos_act]\n",
    "\n",
    "                dataset.extend(pos_set)\n",
    "\n",
    "                for g in gold_facts:\n",
    "                    if len(g) <= 1:\n",
    "                        state = [q[\"query\"], context[g[0]]]\n",
    "\n",
    "                        pos_act = eos\n",
    "                        neg_act = context\n",
    "                        item = [state, pos_act, 1]\n",
    "                        dataset.append(item)\n",
    "                        dataset.extend([[state, n, 0] for n in neg_act])\n",
    "                    else:\n",
    "                        g_0 = g[0]\n",
    "                        g_1 = g[1]\n",
    "\n",
    "                        state = [q[\"query\"], context[g_0]]\n",
    "                        pos_act = context[g_1]\n",
    "                        neg_act = [x for i, x in enumerate(context) if i != g_1]\n",
    "                        item = [state, pos_act, 1]\n",
    "                        dataset.append(item)\n",
    "                        dataset.extend([[state, n, 0] for n in neg_act])\n",
    "\n",
    "                        state = [q[\"query\"], context[g_1]]\n",
    "                        pos_act = context[g_0]\n",
    "                        neg_act = [x for i, x in enumerate(context) if i != g_0]\n",
    "                        item = [state, pos_act, 1]\n",
    "                        dataset.append(item)\n",
    "                        dataset.extend([[state, n, 0] for n in neg_act])\n",
    "\n",
    "                        state = [q[\"query\"], context[g_0], context[g_1]]\n",
    "                        pos_act = eos\n",
    "                        neg_act = context\n",
    "                        item = [state, pos_act, 1]\n",
    "                        dataset.append(item)\n",
    "                        dataset.extend([[state, n, 0] for n in neg_act])\n",
    "\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def prepare_tokenizer(self,tokenizer):\n",
    "        special_tokens = []\n",
    "        special_tokens.extend([\"<sep>\", \"<SEP>\", \"<eos>\", \"[SEP]\"])\n",
    "        tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b9e02e",
   "metadata": {},
   "source": [
    "# Prediction Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8688060a",
   "metadata": {},
   "source": [
    "### This module will take the path where you have the trained model along with path of Dev and Test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2997c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "class Prediction:\n",
    "    def __init__(self,input_path,model_path,threshold):\n",
    "        self.input_path = input_path\n",
    "        self.model_path = model_path\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    \n",
    "    def find_pred(self,test_file):\n",
    "        ssg = SSG()\n",
    "\n",
    "        folder = self.input_path\n",
    "        batch_size = 32\n",
    "\n",
    "        model_path = self.model_path\n",
    "        device = \"cuda:0\"\n",
    "\n",
    "        model = SentenceTransformer(model_path, device=device)\n",
    "\n",
    "        thresholds = self.threshold\n",
    "\n",
    "        names = test_file\n",
    "\n",
    "        softmax = nn.Softmax()\n",
    "        for threshold in thresholds:\n",
    "            for name in names:\n",
    "                data_file = folder + \"/\" + name + \".jsonl\"\n",
    "\n",
    "                outfile = folder + \"/\" + name + \"_\" + str(threshold) + \"_ssg_sup.json\"\n",
    "                dataset = ssg.read_NDB(data_file)\n",
    "                ssg_data = []\n",
    "\n",
    "                db_count = 0\n",
    "                for d in dataset:\n",
    "\n",
    "                    questions = d[1]\n",
    "                    ctx = d[0]\n",
    "\n",
    "                    ctx.insert(0, \"<eos>\")\n",
    "                    ctx_reps = model.encode(ctx)\n",
    "                    q_count = 0\n",
    "                    for q in questions:\n",
    "\n",
    "                        states = [[[-1, q[\"query\"]]]]\n",
    "                        new_states = []\n",
    "                        final_sets = []\n",
    "                        a_reps = ctx_reps[0: q[\"height\"] + 2]\n",
    "\n",
    "                        for t in range(2):\n",
    "\n",
    "                            while states:\n",
    "                                state = states.pop(0)\n",
    "\n",
    "                                state_text = [s[1] for s in state]\n",
    "                                s_text = [\"[SEP]\".join(state_text)]\n",
    "                                s_reps = model.encode(s_text)\n",
    "\n",
    "                                cos_scores = util.pytorch_cos_sim(s_reps, a_reps)[0]\n",
    "                                cos_scores = cos_scores.cpu()\n",
    "\n",
    "                                next_actions = np.nonzero(cos_scores > threshold).squeeze(1)\n",
    "\n",
    "                                next_actions = next_actions.tolist()\n",
    "\n",
    "                                if not next_actions:\n",
    "                                    st = state.copy()\n",
    "                                    final_sets.append(st[1:])\n",
    "\n",
    "                                for a in next_actions:\n",
    "                                    if a == 0:\n",
    "                                        st = state.copy()\n",
    "                                        final_sets.append(st[1:])\n",
    "                                    else:\n",
    "                                        pre_acts = [pre_act[0] for pre_act in state[1:]]\n",
    "                                        if (a - 1) not in pre_acts:\n",
    "                                            new_state = state.copy()\n",
    "                                            new_state.append([a - 1, ctx[a]])\n",
    "                                            new_states.append(new_state)\n",
    "                            states = new_states\n",
    "                            new_states = []\n",
    "\n",
    "                        for s in states:\n",
    "                            st = s.copy()\n",
    "                            facts = st[1:]\n",
    "                            if (\n",
    "                                facts not in final_sets\n",
    "                                and [facts[1], facts[0]] not in final_sets\n",
    "                            ):\n",
    "                                final_sets.append(st[1:])\n",
    "                        data = {}\n",
    "                        data[\"db_id\"] = db_count\n",
    "                        data[\"question_id\"] = q_count\n",
    "                        data[\"query\"] = q[\"query\"]\n",
    "                        data[\"context_height\"] = q[\"height\"]\n",
    "                        data[\"gold_facts\"] = q[\"facts\"]\n",
    "                        data[\"answer\"] = q[\"answer\"]\n",
    "                        data[\"metadata\"] = {\n",
    "                            \"relation_type\": q[\"relation\"],\n",
    "                            \"query_type\": q[\"type\"],\n",
    "                        }\n",
    "                        data[\"ssg_output\"] = final_sets\n",
    "\n",
    "                        ssg_data.append(data)\n",
    "                        q_count = q_count + 1\n",
    "\n",
    "                    db_count = db_count + 1\n",
    "\n",
    "                with open(outfile, \"w\") as out_file:\n",
    "                    json.dump(ssg_data, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bfe5a5",
   "metadata": {},
   "source": [
    "# Evaluating Precision, Recall, Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17327e58",
   "metadata": {},
   "source": [
    "### This module will take the file generated from the previous cell for each of Dev and Test\n",
    "### After that it will compute accuracy measures by treating the original Dev and Test file as base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24a991d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "class evaluate:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def find_matches(self,a_set, a_set_of_sets):\n",
    "        exact = 0\n",
    "        soft = 0\n",
    "        found = False\n",
    "        for s in a_set_of_sets:\n",
    "            s_set = set(s)\n",
    "            if a_set == s_set:\n",
    "                exact = 1\n",
    "                soft = 1\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            for s in a_set_of_sets:\n",
    "                s_set = set(s)\n",
    "                if a_set <= s_set:\n",
    "                    soft = 1\n",
    "                    break\n",
    "\n",
    "        return exact, soft\n",
    "\n",
    "\n",
    "    def evaluate_ndb_with_ssg(self,data_file):\n",
    "        with open(data_file) as json_file:\n",
    "            data = json.load(json_file)\n",
    "\n",
    "        counter = 0\n",
    "\n",
    "        Ps_soft = {}\n",
    "        Rs_soft = {}\n",
    "\n",
    "        Ps_exact = {}\n",
    "        Rs_exact = {}\n",
    "        \n",
    "        Acc_soft = {}\n",
    "        Acc_exact = {}\n",
    "        \n",
    "        Relation_Ps_soft = {}\n",
    "        Relation_Rs_soft = {}\n",
    "\n",
    "        Relation_Ps_exact = {}\n",
    "        Relation_Rs_exact = {}\n",
    "        \n",
    "        Relation_Acc_soft = {}\n",
    "        Relation_Acc_exact = {}\n",
    "\n",
    "        C = {}\n",
    "        Relation_C = {}\n",
    "\n",
    "        for d in data:\n",
    "            counter = counter + 1\n",
    "\n",
    "            gold_facts = d[\"gold_facts\"]\n",
    "            ssg_output = [[f[0] for f in ss] for ss in d[\"ssg_output\"]]\n",
    "            \n",
    "            remove_lst = []\n",
    "            for s in ssg_output:\n",
    "                if (\n",
    "                    len(s) > 1\n",
    "                    and [s[1], s[0]] in ssg_output\n",
    "                    and [s[1], s[0]] not in remove_lst\n",
    "                ):\n",
    "                    remove_lst.append(s)\n",
    "            for r in remove_lst:\n",
    "                ssg_output.remove(r)\n",
    "            answer = d[\"answer\"]\n",
    "            q_type = d[\"metadata\"][\"query_type\"]\n",
    "\n",
    "            if \"complex\" in q_type:\n",
    "                q_type = \"join\"\n",
    "            if \"arg\" in q_type or \"min\" in q_type or \"max\" in q_type:\n",
    "                q_type = \"min/max\"\n",
    "            if q_type not in Ps_soft:\n",
    "                P_soft = 0\n",
    "                P_exact = 0\n",
    "                R_soft = 0\n",
    "                R_exact = 0\n",
    "                A_soft = 0\n",
    "                A_exact = 0\n",
    "                c = 1\n",
    "            else:\n",
    "                P_soft = Ps_soft[q_type]\n",
    "                R_soft = Rs_soft[q_type]\n",
    "                P_exact = Ps_exact[q_type]\n",
    "                R_exact = Rs_exact[q_type]\n",
    "                A_soft = Acc_soft[q_type]\n",
    "                A_exact = Acc_exact[q_type]\n",
    "                c = C[q_type] + 1\n",
    "\n",
    "            relation_type = d[\"metadata\"][\"relation_type\"]\n",
    "            if relation_type not in Relation_Ps_soft:\n",
    "                Relation_P_soft = 0\n",
    "                Relation_P_exact = 0\n",
    "                Relation_R_soft = 0\n",
    "                Relation_R_exact = 0\n",
    "                Relation_A_soft = 0\n",
    "                Relation_A_exact = 0\n",
    "                Relation_c = 1\n",
    "            else:\n",
    "                Relation_P_soft = Relation_Ps_soft[relation_type]\n",
    "                Relation_R_soft = Relation_Rs_soft[relation_type]\n",
    "                Relation_P_exact = Relation_Ps_exact[relation_type]\n",
    "                Relation_R_exact = Relation_Rs_exact[relation_type]\n",
    "                Relation_A_soft = Relation_Acc_soft[relation_type]\n",
    "                Relation_A_exact = Relation_Acc_exact[relation_type]\n",
    "                Relation_c = Relation_C[relation_type] + 1\n",
    "\n",
    "            \n",
    "            ssg_count = 0\n",
    "            gold_count = 0\n",
    "            total_soft = 0\n",
    "            total_exact = 0\n",
    "            acc_soft = 0\n",
    "            acc_exact = 0\n",
    "\n",
    "            \n",
    "            if len(ssg_output) == 0:\n",
    "                total_soft = 1\n",
    "                total_exact = 1\n",
    "                ssg_count = 1\n",
    "                \n",
    "            \n",
    "            for s in ssg_output:\n",
    "                ssg_count = ssg_count + 1\n",
    "                \n",
    "                if s in gold_facts:\n",
    "                    acc_soft+=1\n",
    "                    acc_exact+=1\n",
    "\n",
    "                if s in gold_facts or len(s) == 0:\n",
    "                    total_soft = total_soft + 1\n",
    "                    total_exact = total_exact + 1\n",
    "                else:\n",
    "                    if len(s) > 1 and [s[1], s[0]] in gold_facts:\n",
    "                        total_soft = total_soft + 1\n",
    "                        total_exact = total_exact + 1\n",
    "                        acc_soft+=1\n",
    "                        acc_exact+=1\n",
    "                    else:\n",
    "                        for gold_s in gold_facts:\n",
    "                            if set(gold_s) <= set(s):\n",
    "                                total_soft = total_soft + 1\n",
    "                                acc_soft+=1\n",
    "                                break\n",
    "            P_soft = P_soft + total_soft / ssg_count\n",
    "            P_exact = P_exact + total_exact / ssg_count\n",
    "            if acc_soft>1:\n",
    "                acc_soft=1\n",
    "            if acc_exact>1:\n",
    "                acc_exact=1\n",
    "            A_soft = A_soft + acc_soft\n",
    "            A_exact = A_exact + acc_exact\n",
    "\n",
    "\n",
    "            Relation_P_soft = Relation_P_soft + total_soft / ssg_count\n",
    "            Relation_P_exact = Relation_P_exact + total_exact / ssg_count\n",
    "            \n",
    "            Relation_A_soft+=acc_soft\n",
    "            Relation_A_exact+=acc_exact\n",
    "            \n",
    "            total_exact = 0\n",
    "            total_soft = 0\n",
    "\n",
    "            # Recall\n",
    "            if len(gold_facts) == 0 or answer == \"None\":\n",
    "                total_soft = 1\n",
    "                total_exact = 1\n",
    "                gold_count = 1\n",
    "            else:\n",
    "                for g in gold_facts:\n",
    "                    gold_count = gold_count + 1\n",
    "                    exact, soft = self.find_matches(set(g), ssg_output)\n",
    "                    total_soft = total_soft + soft\n",
    "                    total_exact = total_exact + exact\n",
    "\n",
    "\n",
    "            R_soft = R_soft + total_soft / gold_count\n",
    "            R_exact = R_exact + total_exact / gold_count\n",
    "\n",
    "            Relation_R_soft = Relation_R_soft + total_soft / gold_count\n",
    "            Relation_R_exact = Relation_R_exact + total_exact / gold_count\n",
    "\n",
    "            Ps_exact[q_type] = P_exact\n",
    "            Rs_exact[q_type] = R_exact\n",
    "            Ps_soft[q_type] = P_soft\n",
    "            Rs_soft[q_type] = R_soft\n",
    "            Acc_soft[q_type] = A_soft\n",
    "            Acc_exact[q_type] = A_exact\n",
    "            C[q_type] = c\n",
    "\n",
    "            Relation_Ps_exact[relation_type] = Relation_P_exact\n",
    "            Relation_Rs_exact[relation_type] = Relation_R_exact\n",
    "            Relation_Ps_soft[relation_type] = Relation_P_soft\n",
    "            Relation_Rs_soft[relation_type] = Relation_R_soft\n",
    "            Relation_Acc_soft[relation_type] = Relation_A_soft\n",
    "            Relation_Acc_exact[relation_type] = Relation_A_exact\n",
    "            Relation_C[relation_type] = Relation_c\n",
    "\n",
    "\n",
    "\n",
    "        total_p_exact = 0\n",
    "        total_r_exact = 0\n",
    "        total_p_soft = 0\n",
    "        total_r_soft = 0\n",
    "        total_a_soft = 0\n",
    "        total_a_exact = 0\n",
    "        total_c = 0\n",
    "        \n",
    "        print(\"##########----Query Type Accuracy measures----##########\")\n",
    "        print('\\n')\n",
    "        \n",
    "        for t in Ps_exact:\n",
    "            print(t + \":\")\n",
    "            print('Exact Precision',round((Ps_exact[t] / C[t])*100, 2),'Exact Recall', round((Rs_exact[t] / C[t])*100,2))\n",
    "            print('Soft Precision',round((Ps_soft[t] / C[t])*100,2), 'Soft Recall',round((Rs_soft[t] / C[t])*100,2))\n",
    "            print(\"Soft Accuracy\",round((Acc_soft[t]/C[t])*100,2), \"Exact Accuracy\",round((Acc_exact[t] / C[t])*100,2))\n",
    "            total_c = total_c + C[t]\n",
    "            total_r_exact = total_r_exact + Rs_exact[t]\n",
    "            total_p_exact = total_p_exact + Ps_exact[t]\n",
    "            total_r_soft = total_r_soft + Rs_soft[t]\n",
    "            total_p_soft = total_p_soft + Ps_soft[t]\n",
    "            total_a_soft =total_a_soft + Acc_soft[t]\n",
    "            total_a_exact =total_a_exact + Acc_exact[t]\n",
    "\n",
    "        print(\"Total: \")\n",
    "        print('Exact Precision',round((total_p_exact / total_c)*100,2), 'Exact Recall',round((total_r_exact / total_c)*100,2))\n",
    "        print('Soft Precision',round((total_p_soft / total_c)*100,2), 'Soft Recall',round((total_r_soft / total_c)*100,2))\n",
    "        print('Soft Accuracy',round((total_a_soft / total_c)*100,2), 'Exact Accuracy',round((total_a_exact / total_c)*100,2))\n",
    "        \n",
    "        Relation_total_p_exact = 0\n",
    "        Relation_total_r_exact = 0\n",
    "        Relation_total_p_soft = 0\n",
    "        Relation_total_r_soft = 0\n",
    "        Relation_total_a_soft = 0\n",
    "        Relation_total_a_exact = 0\n",
    "        Relation_total_c = 0\n",
    "        \n",
    "        print('\\n')\n",
    "        print('#########----Relation Accuracy measures----##########')\n",
    "        for t in Relation_Ps_exact:\n",
    "            print(t + \":\")\n",
    "            print('Exact Precision',round((Relation_Ps_exact[t] / Relation_C[t])*100, 2),'Exact Recall', round((Relation_Rs_exact[t] / Relation_C[t])*100,2))\n",
    "            print('Soft Precision',round((Relation_Ps_soft[t] / Relation_C[t])*100,2), 'Soft Recall',round((Relation_Rs_soft[t] / Relation_C[t])*100,2))\n",
    "            print(\"Soft Accuracy\",round((Relation_Acc_soft[t]/Relation_C[t])*100,2), \"Exact Accuracy\",round((Relation_Acc_exact[t] / Relation_C[t])*100,2))\n",
    "            Relation_total_c = Relation_total_c + 1\n",
    "            Relation_total_r_exact = Relation_total_r_exact + round((Relation_Rs_exact[t] / Relation_C[t])*100,2)\n",
    "            Relation_total_p_exact = Relation_total_p_exact + round((Relation_Ps_exact[t] / Relation_C[t])*100, 2)\n",
    "            Relation_total_r_soft = Relation_total_r_soft + round((Relation_Rs_soft[t] / Relation_C[t])*100,2)\n",
    "            Relation_total_p_soft = Relation_total_p_soft + round((Relation_Ps_soft[t] / Relation_C[t])*100,2)\n",
    "            Relation_total_a_soft =Relation_total_a_soft + round((Relation_Acc_soft[t]/Relation_C[t])*100,2)\n",
    "            Relation_total_a_exact =Relation_total_a_exact + round((Relation_Acc_exact[t] / Relation_C[t])*100,2)\n",
    "\n",
    "        print(\"Total: \")\n",
    "        print('Exact Precision',round((Relation_total_p_exact / Relation_total_c),2), 'Exact Recall',round((Relation_total_r_exact / Relation_total_c),2))\n",
    "        print('Soft Precision',round((Relation_total_p_soft / Relation_total_c),2), 'Soft Recall',round((Relation_total_r_soft / Relation_total_c),2))\n",
    "        print('Soft Accuracy',round((Relation_total_a_soft / Relation_total_c),2), 'Exact Accuracy',round((Relation_total_a_exact / Relation_total_c),2))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da7a429",
   "metadata": {},
   "source": [
    "# Prediction for original task (KELM Data) trained on itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050a9558",
   "metadata": {},
   "source": [
    "### The Prediction class takes 3 parameters\n",
    "### 1. Input File path\n",
    "### 2. Model File path\n",
    "### 3. Threshold value for cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8c27522",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Prediction(\"5k_data\",\"5k_data/Model_Data\",[0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb8719b",
   "metadata": {},
   "source": [
    "### The find_pred method takes 1 parameter\n",
    "### 1. A list of file name whose prediction we want from the model\n",
    "### The file name path is already provided in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "107a7eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.find_pred([\"balanced_dev\",\"balanced_test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f01636",
   "metadata": {},
   "source": [
    "### Evaluation class has method evaluate_ndb_with_ssg\n",
    "### Which takes the exact location of the Prediction file generated from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9636ac0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########----Query Type Accuracy measures----##########\n",
      "\n",
      "\n",
      "set:\n",
      "Exact Precision 74.33 Exact Recall 91.67\n",
      "Soft Precision 74.33 Soft Recall 91.67\n",
      "Soft Accuracy 54.17 Exact Accuracy 54.17\n",
      "count:\n",
      "Exact Precision 52.23 Exact Recall 92.31\n",
      "Soft Precision 52.23 Soft Recall 92.31\n",
      "Soft Accuracy 61.54 Exact Accuracy 61.54\n",
      "min/max:\n",
      "Exact Precision 58.66 Exact Recall 88.51\n",
      "Soft Precision 58.66 Soft Recall 88.51\n",
      "Soft Accuracy 75.86 Exact Accuracy 75.86\n",
      "bool:\n",
      "Exact Precision 47.66 Exact Recall 73.08\n",
      "Soft Precision 47.66 Soft Recall 73.08\n",
      "Soft Accuracy 42.31 Exact Accuracy 42.31\n",
      "Total: \n",
      "Exact Precision 57.93 Exact Recall 86.35\n",
      "Soft Precision 57.93 Soft Recall 86.35\n",
      "Soft Accuracy 59.05 Exact Accuracy 59.05\n",
      "\n",
      "\n",
      "#########----Relation Accuracy measures----##########\n",
      "P47:\n",
      "Exact Precision 35.0 Exact Recall 100.0\n",
      "Soft Precision 35.0 Soft Recall 100.0\n",
      "Soft Accuracy 75.0 Exact Accuracy 75.0\n",
      "P21:\n",
      "Exact Precision 74.32 Exact Recall 96.55\n",
      "Soft Precision 74.32 Soft Recall 96.55\n",
      "Soft Accuracy 86.21 Exact Accuracy 86.21\n",
      "P106:\n",
      "Exact Precision 51.94 Exact Recall 74.6\n",
      "Soft Precision 51.94 Soft Recall 74.6\n",
      "Soft Accuracy 57.14 Exact Accuracy 57.14\n",
      "P19:\n",
      "Exact Precision 71.12 Exact Recall 85.71\n",
      "Soft Precision 71.12 Soft Recall 85.71\n",
      "Soft Accuracy 57.14 Exact Accuracy 57.14\n",
      "P108:\n",
      "Exact Precision 8.33 Exact Recall 66.67\n",
      "Soft Precision 8.33 Soft Recall 66.67\n",
      "Soft Accuracy 33.33 Exact Accuracy 33.33\n",
      "P27:\n",
      "Exact Precision 56.56 Exact Recall 86.67\n",
      "Soft Precision 56.56 Soft Recall 86.67\n",
      "Soft Accuracy 46.67 Exact Accuracy 46.67\n",
      "P54:\n",
      "Exact Precision 75.0 Exact Recall 90.0\n",
      "Soft Precision 75.0 Soft Recall 90.0\n",
      "Soft Accuracy 30.0 Exact Accuracy 30.0\n",
      "P50:\n",
      "Exact Precision 100.0 Exact Recall 100.0\n",
      "Soft Precision 100.0 Soft Recall 100.0\n",
      "Soft Accuracy 0.0 Exact Accuracy 0.0\n",
      "P69:\n",
      "Exact Precision 43.06 Exact Recall 100.0\n",
      "Soft Precision 43.06 Soft Recall 100.0\n",
      "Soft Accuracy 66.67 Exact Accuracy 66.67\n",
      "P1082:\n",
      "Exact Precision 24.07 Exact Recall 66.67\n",
      "Soft Precision 24.07 Soft Recall 66.67\n",
      "Soft Accuracy 33.33 Exact Accuracy 33.33\n",
      "Total: \n",
      "Exact Precision 53.94 Exact Recall 86.69\n",
      "Soft Precision 53.94 Soft Recall 86.69\n",
      "Soft Accuracy 48.55 Exact Accuracy 48.55\n"
     ]
    }
   ],
   "source": [
    "evalu = evaluate()\n",
    "evalu.evaluate_ndb_with_ssg(\"5k_data/balanced_test_0.7_ssg_sup.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0589d91",
   "metadata": {},
   "source": [
    "# Inference with Original(KELM Data) + Table Data(Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1785d9df",
   "metadata": {},
   "source": [
    "### Here we are running the original+table data through the model trained on Original KELM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3229c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Prediction(\"Inference_5k_datatable\",\"Inference_5k_datatable/Model_Data\",[0.7])\n",
    "pred.find_pred([\"combined_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd92007d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########----Query Type Accuracy measures----##########\n",
      "\n",
      "\n",
      "min/max:\n",
      "Exact Precision 52.53 Exact Recall 90.95\n",
      "Soft Precision 54.53 Soft Recall 91.24\n",
      "Soft Accuracy 72.65 Exact Accuracy 72.65\n",
      "set:\n",
      "Exact Precision 42.7 Exact Recall 94.32\n",
      "Soft Precision 46.41 Soft Recall 94.32\n",
      "Soft Accuracy 69.32 Exact Accuracy 69.32\n",
      "bool:\n",
      "Exact Precision 35.73 Exact Recall 90.59\n",
      "Soft Precision 40.38 Soft Recall 92.94\n",
      "Soft Accuracy 72.94 Exact Accuracy 70.59\n",
      "count:\n",
      "Exact Precision 36.56 Exact Recall 88.63\n",
      "Soft Precision 44.3 Soft Recall 94.12\n",
      "Soft Accuracy 67.06 Exact Accuracy 62.35\n",
      "Total: \n",
      "Exact Precision 42.8 Exact Recall 91.13\n",
      "Soft Precision 47.1 Soft Recall 93.0\n",
      "Soft Accuracy 70.67 Exact Accuracy 69.07\n",
      "\n",
      "\n",
      "#########----Relation Accuracy measures----##########\n",
      "P69:\n",
      "Exact Precision 38.43 Exact Recall 90.74\n",
      "Soft Precision 40.65 Soft Recall 90.74\n",
      "Soft Accuracy 59.26 Exact Accuracy 59.26\n",
      "P21:\n",
      "Exact Precision 59.66 Exact Recall 96.69\n",
      "Soft Precision 61.72 Soft Recall 96.69\n",
      "Soft Accuracy 88.24 Exact Accuracy 88.24\n",
      "P27:\n",
      "Exact Precision 41.26 Exact Recall 94.44\n",
      "Soft Precision 45.22 Soft Recall 94.44\n",
      "Soft Accuracy 83.33 Exact Accuracy 83.33\n",
      "P1110:\n",
      "Exact Precision 62.5 Exact Recall 87.5\n",
      "Soft Precision 62.5 Soft Recall 87.5\n",
      "Soft Accuracy 12.5 Exact Accuracy 12.5\n",
      "P106:\n",
      "Exact Precision 33.14 Exact Recall 90.0\n",
      "Soft Precision 34.64 Soft Recall 90.0\n",
      "Soft Accuracy 70.0 Exact Accuracy 70.0\n",
      "P19:\n",
      "Exact Precision 35.06 Exact Recall 99.19\n",
      "Soft Precision 36.82 Soft Recall 99.19\n",
      "Soft Accuracy 65.85 Exact Accuracy 65.85\n",
      "P1082:\n",
      "Exact Precision 48.3 Exact Recall 89.47\n",
      "Soft Precision 50.4 Soft Recall 89.47\n",
      "Soft Accuracy 63.16 Exact Accuracy 63.16\n",
      "P47:\n",
      "Exact Precision 43.53 Exact Recall 100.0\n",
      "Soft Precision 45.45 Soft Recall 100.0\n",
      "Soft Accuracy 65.38 Exact Accuracy 65.38\n",
      "P118:\n",
      "Exact Precision 10.83 Exact Recall 41.67\n",
      "Soft Precision 35.39 Soft Recall 70.83\n",
      "Soft Accuracy 50.0 Exact Accuracy 25.0\n",
      "P54:\n",
      "Exact Precision 39.87 Exact Recall 84.33\n",
      "Soft Precision 51.25 Soft Recall 89.0\n",
      "Soft Accuracy 74.0 Exact Accuracy 70.0\n",
      "P108:\n",
      "Exact Precision 18.0 Exact Recall 100.0\n",
      "Soft Precision 26.0 Soft Recall 100.0\n",
      "Soft Accuracy 80.0 Exact Accuracy 80.0\n",
      "P1174:\n",
      "Exact Precision 60.0 Exact Recall 100.0\n",
      "Soft Precision 60.0 Soft Recall 100.0\n",
      "Soft Accuracy 100.0 Exact Accuracy 100.0\n",
      "P50:\n",
      "Exact Precision 67.42 Exact Recall 100.0\n",
      "Soft Precision 68.94 Soft Recall 100.0\n",
      "Soft Accuracy 45.45 Exact Accuracy 45.45\n",
      "P26:\n",
      "Exact Precision 43.75 Exact Recall 100.0\n",
      "Soft Precision 43.75 Soft Recall 100.0\n",
      "Soft Accuracy 75.0 Exact Accuracy 75.0\n",
      "P20:\n",
      "Exact Precision 100.0 Exact Recall 100.0\n",
      "Soft Precision 100.0 Soft Recall 100.0\n",
      "Soft Accuracy 50.0 Exact Accuracy 50.0\n",
      "Total: \n",
      "Exact Precision 46.78 Exact Recall 91.6\n",
      "Soft Precision 50.85 Soft Recall 93.86\n",
      "Soft Accuracy 65.48 Exact Accuracy 63.54\n"
     ]
    }
   ],
   "source": [
    "evalu = evaluate()\n",
    "evalu.evaluate_ndb_with_ssg(\"Inference_5k_datatable/combined_data_0.6_ssg_sup.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6158204e",
   "metadata": {},
   "source": [
    "# Inference with only Table Data(Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d1258",
   "metadata": {},
   "source": [
    "### Here we are running the table data through the model trained on Original KELM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7799b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Prediction(\"Inference_original_onlytable\",\"Inference_original_onlytable/Model_Data\",[0.7])\n",
    "pred.find_pred([\"combined_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d96b0f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########----Query Type Accuracy measures----##########\n",
      "\n",
      "\n",
      "min/max:\n",
      "Exact Precision 60.97 Exact Recall 20.78\n",
      "Soft Precision 84.27 Soft Recall 36.03\n",
      "Soft Accuracy 38.33 Exact Accuracy 21.67\n",
      "bool:\n",
      "Exact Precision 19.52 Exact Recall 55.71\n",
      "Soft Precision 58.8 Soft Recall 94.29\n",
      "Soft Accuracy 78.57 Exact Accuracy 42.86\n",
      "count:\n",
      "Exact Precision 58.03 Exact Recall 47.62\n",
      "Soft Precision 84.09 Soft Recall 67.86\n",
      "Soft Accuracy 41.07 Exact Accuracy 21.43\n",
      "set:\n",
      "Exact Precision 44.62 Exact Recall 61.83\n",
      "Soft Precision 72.77 Soft Recall 75.81\n",
      "Soft Accuracy 54.84 Exact Accuracy 41.94\n",
      "Total: \n",
      "Exact Precision 44.52 Exact Recall 46.96\n",
      "Soft Precision 74.17 Soft Recall 69.6\n",
      "Soft Accuracy 54.44 Exact Accuracy 32.66\n",
      "\n",
      "\n",
      "#########----Relation Accuracy measures----##########\n",
      "P54:\n",
      "Exact Precision 40.35 Exact Recall 48.51\n",
      "Soft Precision 72.07 Soft Recall 71.49\n",
      "Soft Accuracy 60.1 Exact Accuracy 38.42\n",
      "P118:\n",
      "Exact Precision 63.33 Exact Recall 40.0\n",
      "Soft Precision 83.61 Soft Recall 61.11\n",
      "Soft Accuracy 28.89 Exact Accuracy 6.67\n",
      "Total: \n",
      "Exact Precision 51.84 Exact Recall 44.25\n",
      "Soft Precision 77.84 Soft Recall 66.3\n",
      "Soft Accuracy 44.5 Exact Accuracy 22.55\n"
     ]
    }
   ],
   "source": [
    "evalu = evaluate()\n",
    "evalu.evaluate_ndb_with_ssg(\"Inference_original_onlytable/combined_data_0.7_ssg_sup.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb98559",
   "metadata": {},
   "source": [
    "# Prediction for Original(KELM Data) + Table Data(Wikipedia) trained on itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8f0d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Prediction(\"5k_datatable\",\"5k_datatable/Model_Data\",[0.7])\n",
    "pred.find_pred([\"balanced_dev\",\"balanced_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd9779c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########----Query Type Accuracy measures----##########\n",
      "\n",
      "\n",
      "set:\n",
      "Exact Precision 58.55 Exact Recall 83.87\n",
      "Soft Precision 58.55 Soft Recall 83.87\n",
      "Soft Accuracy 64.52 Exact Accuracy 64.52\n",
      "bool:\n",
      "Exact Precision 33.08 Exact Recall 100.0\n",
      "Soft Precision 33.08 Soft Recall 100.0\n",
      "Soft Accuracy 84.21 Exact Accuracy 84.21\n",
      "count:\n",
      "Exact Precision 63.52 Exact Recall 86.36\n",
      "Soft Precision 63.52 Soft Recall 86.36\n",
      "Soft Accuracy 63.64 Exact Accuracy 63.64\n",
      "min/max:\n",
      "Exact Precision 65.46 Exact Recall 87.85\n",
      "Soft Precision 65.46 Soft Recall 87.85\n",
      "Soft Accuracy 68.42 Exact Accuracy 68.42\n",
      "Total: \n",
      "Exact Precision 57.53 Exact Recall 88.53\n",
      "Soft Precision 57.53 Soft Recall 88.53\n",
      "Soft Accuracy 69.09 Exact Accuracy 69.09\n",
      "\n",
      "\n",
      "#########----Relation Accuracy measures----##########\n",
      "P106:\n",
      "Exact Precision 42.43 Exact Recall 100.0\n",
      "Soft Precision 42.43 Soft Recall 100.0\n",
      "Soft Accuracy 82.61 Exact Accuracy 82.61\n",
      "P27:\n",
      "Exact Precision 29.68 Exact Recall 100.0\n",
      "Soft Precision 29.68 Soft Recall 100.0\n",
      "Soft Accuracy 90.0 Exact Accuracy 90.0\n",
      "P118:\n",
      "Exact Precision 53.12 Exact Recall 41.67\n",
      "Soft Precision 53.12 Soft Recall 41.67\n",
      "Soft Accuracy 37.5 Exact Accuracy 37.5\n",
      "P26:\n",
      "Exact Precision 75.0 Exact Recall 100.0\n",
      "Soft Precision 75.0 Soft Recall 100.0\n",
      "Soft Accuracy 100.0 Exact Accuracy 100.0\n",
      "P21:\n",
      "Exact Precision 63.11 Exact Recall 100.0\n",
      "Soft Precision 63.11 Soft Recall 100.0\n",
      "Soft Accuracy 94.12 Exact Accuracy 94.12\n",
      "P19:\n",
      "Exact Precision 54.06 Exact Recall 87.06\n",
      "Soft Precision 54.06 Soft Recall 87.06\n",
      "Soft Accuracy 58.82 Exact Accuracy 58.82\n",
      "P54:\n",
      "Exact Precision 72.27 Exact Recall 84.09\n",
      "Soft Precision 72.27 Soft Recall 84.09\n",
      "Soft Accuracy 90.91 Exact Accuracy 90.91\n",
      "P1082:\n",
      "Exact Precision 58.93 Exact Recall 100.0\n",
      "Soft Precision 58.93 Soft Recall 100.0\n",
      "Soft Accuracy 50.0 Exact Accuracy 50.0\n",
      "P69:\n",
      "Exact Precision 66.67 Exact Recall 60.0\n",
      "Soft Precision 66.67 Soft Recall 60.0\n",
      "Soft Accuracy 20.0 Exact Accuracy 20.0\n",
      "P50:\n",
      "Exact Precision 91.67 Exact Recall 83.33\n",
      "Soft Precision 91.67 Soft Recall 83.33\n",
      "Soft Accuracy 33.33 Exact Accuracy 33.33\n",
      "P47:\n",
      "Exact Precision 68.75 Exact Recall 100.0\n",
      "Soft Precision 68.75 Soft Recall 100.0\n",
      "Soft Accuracy 50.0 Exact Accuracy 50.0\n",
      "P1110:\n",
      "Exact Precision 100.0 Exact Recall 100.0\n",
      "Soft Precision 100.0 Soft Recall 100.0\n",
      "Soft Accuracy 0.0 Exact Accuracy 0.0\n",
      "P20:\n",
      "Exact Precision 100.0 Exact Recall 50.0\n",
      "Soft Precision 100.0 Soft Recall 50.0\n",
      "Soft Accuracy 0.0 Exact Accuracy 0.0\n",
      "Total: \n",
      "Exact Precision 67.36 Exact Recall 85.09\n",
      "Soft Precision 67.36 Soft Recall 85.09\n",
      "Soft Accuracy 54.41 Exact Accuracy 54.41\n"
     ]
    }
   ],
   "source": [
    "evalu = evaluate()\n",
    "evalu.evaluate_ndb_with_ssg(\"5k_datatable/balanced_test_0.7_ssg_sup.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb9eaa",
   "metadata": {},
   "source": [
    "# Inference with only Table Data(Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fe6957",
   "metadata": {},
   "source": [
    "### Here we are running the table data through the model trained on original+table Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "36b1236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Prediction(\"Inference_orig+table_onlytable\",\"Inference_orig+table_onlytable/Model_Data\",[0.7])\n",
    "pred.find_pred([\"combined_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5119bd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########----Query Type Accuracy measures----##########\n",
      "\n",
      "\n",
      "min/max:\n",
      "Exact Precision 46.44 Exact Recall 45.69\n",
      "Soft Precision 64.58 Soft Recall 49.44\n",
      "Soft Accuracy 60.0 Exact Accuracy 55.0\n",
      "bool:\n",
      "Exact Precision 44.94 Exact Recall 85.0\n",
      "Soft Precision 48.51 Soft Recall 85.0\n",
      "Soft Accuracy 70.0 Exact Accuracy 70.0\n",
      "count:\n",
      "Exact Precision 75.16 Exact Recall 73.21\n",
      "Soft Precision 82.59 Soft Recall 73.21\n",
      "Soft Accuracy 46.43 Exact Accuracy 46.43\n",
      "set:\n",
      "Exact Precision 56.34 Exact Recall 65.32\n",
      "Soft Precision 73.58 Soft Recall 77.42\n",
      "Soft Accuracy 56.45 Exact Accuracy 46.77\n",
      "Total: \n",
      "Exact Precision 54.98 Exact Recall 67.91\n",
      "Soft Precision 66.36 Soft Recall 71.84\n",
      "Soft Accuracy 58.87 Exact Accuracy 55.24\n",
      "\n",
      "\n",
      "#########----Relation Accuracy measures----##########\n",
      "P54:\n",
      "Exact Precision 50.29 Exact Recall 69.17\n",
      "Soft Precision 64.2 Soft Recall 73.97\n",
      "Soft Accuracy 65.52 Exact Accuracy 61.08\n",
      "P118:\n",
      "Exact Precision 76.11 Exact Recall 62.22\n",
      "Soft Precision 76.11 Soft Recall 62.22\n",
      "Soft Accuracy 28.89 Exact Accuracy 28.89\n",
      "Total: \n",
      "Exact Precision 63.2 Exact Recall 65.69\n",
      "Soft Precision 70.16 Soft Recall 68.09\n",
      "Soft Accuracy 47.2 Exact Accuracy 44.98\n"
     ]
    }
   ],
   "source": [
    "evalu = evaluate()\n",
    "evalu.evaluate_ndb_with_ssg(\"Inference_orig+table_onlytable/combined_data_0.7_ssg_sup.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c022e",
   "metadata": {},
   "source": [
    "# Prediction for only Tabe Data(Wikipedia) trained on itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4bc632c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Prediction(\"onlytable\",\"onlytable/Model_Data\",[0.7])\n",
    "pred.find_pred([\"balanced_dev\",\"balanced_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4d125d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########----Query Type Accuracy measures----##########\n",
      "\n",
      "\n",
      "set:\n",
      "Exact Precision 49.62 Exact Recall 96.79\n",
      "Soft Precision 53.84 Soft Recall 96.79\n",
      "Soft Accuracy 85.71 Exact Accuracy 85.71\n",
      "min/max:\n",
      "Exact Precision 45.0 Exact Recall 76.6\n",
      "Soft Precision 45.0 Soft Recall 76.6\n",
      "Soft Accuracy 77.78 Exact Accuracy 77.78\n",
      "bool:\n",
      "Exact Precision 35.29 Exact Recall 88.8\n",
      "Soft Precision 35.29 Soft Recall 88.8\n",
      "Soft Accuracy 75.0 Exact Accuracy 75.0\n",
      "count:\n",
      "Exact Precision 51.65 Exact Recall 93.33\n",
      "Soft Precision 51.65 Soft Recall 93.33\n",
      "Soft Accuracy 53.33 Exact Accuracy 53.33\n",
      "Total: \n",
      "Exact Precision 45.13 Exact Recall 86.72\n",
      "Soft Precision 45.95 Soft Recall 86.72\n",
      "Soft Accuracy 73.61 Exact Accuracy 73.61\n",
      "\n",
      "\n",
      "#########----Relation Accuracy measures----##########\n",
      "P54:\n",
      "Exact Precision 46.95 Exact Recall 88.88\n",
      "Soft Precision 47.95 Soft Recall 88.88\n",
      "Soft Accuracy 84.75 Exact Accuracy 84.75\n",
      "P118:\n",
      "Exact Precision 36.86 Exact Recall 76.92\n",
      "Soft Precision 36.86 Soft Recall 76.92\n",
      "Soft Accuracy 23.08 Exact Accuracy 23.08\n",
      "Total: \n",
      "Exact Precision 41.91 Exact Recall 82.9\n",
      "Soft Precision 42.41 Soft Recall 82.9\n",
      "Soft Accuracy 53.91 Exact Accuracy 53.91\n"
     ]
    }
   ],
   "source": [
    "evalu = evaluate()\n",
    "evalu.evaluate_ndb_with_ssg(\"onlytable/balanced_test_0.7_ssg_sup.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f80c9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
